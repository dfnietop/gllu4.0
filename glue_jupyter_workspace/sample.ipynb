{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Code Samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial Config\n",
    "use `%%configure magic` to configure spark since there are some configs that appear to not be affected by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "\"conf\": {\n",
    "\"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "\"spark.sql.hive.convertMetastoreParquet\": \"false\",\n",
    "\"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "\"spark.sql.legacy.pathOptionBehavior.enabled\": \"true\",\n",
    "\"spark.sql.extensions\": \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\"\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually add job params like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.argv.append('--JOB_NAME')\n",
    "sys.argv.append('test_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pip libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Libraries\n",
    "The custom libraries shoud be in the `./extra_python_path` and imported with the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hudi_library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading Hudi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"hudi\").load(f\"s3://<bucket-name>/<path-to-table>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Writing Hudi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from faker import Faker\n",
    "\n",
    "import sys, boto3\n",
    "from awsglue.context import GlueContext, DynamicFrame\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, lit, to_timestamp\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.job import Job\n",
    "import json\n",
    "\n",
    "args = getResolvedOptions(\n",
    "    sys.argv, [\n",
    "        'JOB_NAME'\n",
    "    ],\n",
    ")\n",
    "\n",
    "spark = (SparkSession.builder.config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "         .config('spark.sql.hive.convertMetastoreParquet', 'false') \\\n",
    "         .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.hudi.catalog.HoodieCatalog') \\\n",
    "         .config('spark.sql.extensions', 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension') \\\n",
    "         .config('spark.sql.legacy.pathOptionBehavior.enabled', 'true').getOrCreate())\n",
    "\n",
    "# Create a Spark context and Glue context\n",
    "sc = spark.sparkContext\n",
    "glueContext = GlueContext(sc)\n",
    "job = Job(glueContext)\n",
    "logger = glueContext.get_logger()\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Creating Fake Data with faker lib\n",
    "fake = Faker()\n",
    "Faker.seed(0)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"email\", StringType()),\n",
    "    StructField(\"address\", StringType()),\n",
    "    StructField(\"ts\", StringType()) # Required precombine.field by hudi\n",
    "])\n",
    "\n",
    "data = []\n",
    "for i in range(100):\n",
    "    data.append((fake.uuid4(), fake.name(), fake.email(), fake.address(), fake.date_time_this_month()))\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Hudi settings adjust as needed.\n",
    "hudi_common_settings = {\n",
    "    \"className\" : \"org.apache.hudi\", \n",
    "    \"hoodie.table.name\": \"sample_tb\", # Glue Catalog table name\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n",
    "    \"hoodie.datasource.write.operation\": \"insert_overwrite_table\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"id\", # primary key \n",
    "    \"hoodie.datasource.write.precombine.field\" : \"ts\", # precombined key\n",
    "    \"path\" : \"s3://testbucket-juanamaya/sample_db/sample_tb/\", # S3 target path\n",
    "}\n",
    "hudi_index_settings = {\n",
    "    \"hoodie.index.type\": \"BLOOM\", \n",
    "}\n",
    "hudi_hive_sync_settings = {\n",
    "    \"hoodie.parquet.compression.codec\": \"gzip\",\n",
    "    \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    \"hoodie.datasource.hive_sync.database\": \"sample_db\", # Glue Catalog database name\n",
    "    \"hoodie.datasource.hive_sync.table\": \"sample_tb\", # Glue Catalog table name\n",
    "    \"hoodie.datasource.hive_sync.use_jdbc\": \"false\",\n",
    "    \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "}\n",
    "hudi_cleaner_options = {\n",
    "    \"hoodie.clean.automatic\": \"true\",\n",
    "    \"hoodie.clean.async\": \"true\",\n",
    "    \"hoodie.cleaner.policy\": 'KEEP_LATEST_COMMITS',\n",
    "    'hoodie.cleaner.commits.retained': 10,\n",
    "    \"hoodie-conf hoodie.cleaner.parallelism\": '200',\n",
    "}\n",
    "unpartition_settings = {\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', \n",
    "    'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator',\n",
    "}\n",
    "hudi_final_settings = {**hudi_common_settings, **hudi_index_settings, **hudi_hive_sync_settings, **hudi_cleaner_options, **unpartition_settings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write with Hudi full overwrite table and contents, adjust logic to use upserts, incremental ETL or cdc\n",
    "df.write.format('hudi').options(**hudi_final_settings).mode('Overwrite').save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
